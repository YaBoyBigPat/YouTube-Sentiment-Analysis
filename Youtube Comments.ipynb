{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from IPython.display import JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove API Key Before Publishing to GitHub**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove API key before pushing to GitHub\n",
    "api_key = \"API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can place multiple channel ids in the list, 0 = Open AI, 1 = Kurzgesagt\n",
    "channel_ids = [\"Open AI\", \"Kurzgesagt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httplib2\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  api_service_name = \"youtube\"\n",
    "  api_version = \"v3\"\n",
    "\n",
    "   \n",
    "youtube = build(\n",
    "    api_service_name, api_version, developerKey=api_key)\n",
    "request = youtube.channels().list(\n",
    "    part=\"snippet,contentDetails,statistics\",\n",
    "    id=\",\".join(channel_ids)  # concatenates channel ids with commas\n",
    ")\n",
    "response = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Channel Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_stats(youtube, channel_ids):\n",
    "\n",
    "    request = youtube.channels().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        id=\",\".join(channel_ids)  # concatenates channel ids with commas\n",
    "    )\n",
    "    response = request.execute()\n",
    "    all_data = []\n",
    "    # Loop through each channel and get the stats\n",
    "    for item in response['items']:\n",
    "        data = {\n",
    "            \"channel Name\": item['snippet']['title'],\n",
    "            \"channelid\": item['id'],\n",
    "            \"channel description\": item['snippet']['description'],\n",
    "            \"channel subscribers\": item['statistics']['subscriberCount'],\n",
    "            \"channel video count\": item['statistics']['videoCount'],\n",
    "            \"channel view count\": item['statistics']['viewCount'],\n",
    "            \"uploads id\": item['contentDetails']['relatedPlaylists']['uploads'] # add a check for the key\n",
    "        }\n",
    "        all_data.append(data)\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats = get_channel_stats(youtube, channel_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting Video ID's for Kurzgesagt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploads id from channel stats\n",
    "playlist_id = \"UUsXVk37bltHxD1rDPwtNM8Q\"\n",
    "\n",
    "def get_video_ids(youtube, playlist_id):\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    request = youtube.playlistItems().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        playlistId = playlist_id,\n",
    "        maxResults = 50,\n",
    "\n",
    "    )\n",
    "    response = request.execute()\n",
    "   \n",
    "    for item in response['items']:\n",
    "        video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "    next_page_token = response.get('nextPageToken')\n",
    "    while next_page_token is not None:\n",
    "        request = youtube.playlistItems().list(\n",
    "        part=\"snippet,contentDetails\",\n",
    "        playlistId = playlist_id,\n",
    "        maxResults = 50,\n",
    "        pageToken = next_page_token\n",
    "\n",
    "        )\n",
    "        response = request.execute()\n",
    "    \n",
    "        for item in response['items']:\n",
    "            video_ids.append(item['contentDetails']['videoId'])\n",
    "\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "\n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids = get_video_ids(youtube, playlist_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the Video Stats for the all of the Video's we pulled from Kurzgesagt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, video_ids):\n",
    "    all_video_info = []\n",
    "\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50])\n",
    "        ) \n",
    "        response = request.execute()\n",
    "            \n",
    "        for video in response['items']:\n",
    "            stats_to_keep = {'snippet': ['channelTitle', 'title', 'description', 'tags', 'publishedAt'],\n",
    "                            'statistics': ['viewCount', 'likeCount', 'favoriteCount', 'commentCount'],\n",
    "                            'contentDetails': ['duration', 'definition', 'caption']}\n",
    "            video_info = {}\n",
    "            video_info['video_id'] = video['id']\n",
    "\n",
    "            for s in stats_to_keep:\n",
    "                for v in stats_to_keep[s]:\n",
    "                    try:\n",
    "                        video_info[v] = video[s][v]\n",
    "                    except:\n",
    "                        video_info[v] = None\n",
    "\n",
    "            all_video_info.append(video_info)\n",
    "    \n",
    "    return pd.DataFrame(all_video_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df = get_video_details(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_video_ids = video_ids[0:200]\n",
    "video_details_df = get_video_details(youtube, selected_video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code below to clean the comments before we save them, Skip this and remove the function here \"comments_in_video = [\n",
    "                clean_text()\" to keep the raw comments  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean text two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Origional comments request, get's one page per video, aproximitly 50 comment's per page. We Can Skip this def and use the 3rd one below.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "def get_comments(youtube, video_ids):\n",
    "    all_comments = []\n",
    "\n",
    "# We'll include try a and except to continue if the comments are disabled for a video\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet,replies\",\n",
    "                videoId=video_id,\n",
    "                order=\"relevance\",\n",
    "                maxResults = 50\n",
    "            \n",
    "            )\n",
    "            response = request.execute()\n",
    "        #Can replace original_comments with comments_in_video and remove \"comments_in_video = preprocess_comments(original_comments)\" to go back to original comments\n",
    "            comments_in_video = [\n",
    "                (comment['snippet']['topLevelComment']['snippet']['textOriginal'])\n",
    "                for comment in response['items']\n",
    "            ]\n",
    "             # Preprocess the comments, remove to go back to original comments\n",
    "            #comments_in_video = preprocess_comments(original_comments)\n",
    "\n",
    "            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}\n",
    "\n",
    "            all_comments.append(comments_in_video_info)\n",
    "        except HttpError as e:\n",
    "            print(f\"HttpError for video_id {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(all_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = get_comments(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Def 2) Get's 2 pages of comments approximently 50 comments per page.\n",
    "Can be deleted.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def get_comments(youtube, video_ids):\n",
    "    all_comments = []\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            nextPageToken = None  # Initialize the nextPageToken for each video\n",
    "            page_count = 0  # Initialize a counter for the number of pages processed\n",
    "\n",
    "            while page_count < 2:  # Fetch two pages then stop\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    order=\"relevance\",\n",
    "                    maxResults=50,\n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "\n",
    "                # Extract comments\n",
    "                comments_in_video = [\n",
    "                    comment['snippet']['topLevelComment']['snippet']['textOriginal']\n",
    "                    for comment in response['items']\n",
    "                ]\n",
    "\n",
    "                # Add the comments to your list\n",
    "                all_comments.append({\n",
    "                    'video_id': video_id,\n",
    "                    'comments': comments_in_video\n",
    "                })\n",
    "\n",
    "                # Increment the page counter\n",
    "                page_count += 1\n",
    "\n",
    "                # Check if there is a nextPageToken, and update nextPageToken\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "                if not nextPageToken:\n",
    "                    break  # Exit the loop if no more pages\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"HttpError for video_id {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Combine all comments into a DataFrame\n",
    "    return pd.DataFrame(all_comments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing emojis, currently commented out to keep the origional comments. You can delete this if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "\n",
    "#def remove_emojis(text):\n",
    "   # \"\"\"Remove emojis from the text using regular expression.\"\"\"\n",
    "    #emoji_pattern = re.compile(\"[\"\n",
    "                          # u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                          # u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                          # u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                          # u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                          # u\"\\U00002702-\\U000027B0\"\n",
    "                           #u\"\\U000024C2-\\U0001F251\"\n",
    "                           #u\"\\U0001F900-\\U0001FAFF\"  # Supplemental Symbols and Pictographs + Emoticons\n",
    "                           #u\"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "                           #u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                           #u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                           #u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                           #u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                           #u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                           #u\"\\U0001FA70-\\U0001FAFF\" # Symbols and Pictographs Extended-A\n",
    "                           #u\"\\U0001FB00-\\U0001FBFF\" # Symbols for Legacy Computing\n",
    "                           #\"]+\", flags=re.UNICODE)\n",
    "    #return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Def 3) Use this def to fetch comments, the ones above can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "def get_comments(youtube, video_ids):\n",
    "    all_comments = []\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            nextPageToken = None  # Initialize the nextPageToken for each video\n",
    "            page_count = 0  # Initialize a counter for the number of pages processed\n",
    "\n",
    "            while page_count < 2:  # Fetch two pages then stop\n",
    "                request = youtube.commentThreads().list(\n",
    "                    part=\"snippet,replies\",\n",
    "                    videoId=video_id,\n",
    "                    order=\"relevance\",\n",
    "                    maxResults=50, # Getting about 100 comments per video since we are fetching 2 pages \n",
    "                    pageToken=nextPageToken\n",
    "                )\n",
    "                response = request.execute()\n",
    "\n",
    "                # Extract comments\n",
    "                comments_in_page = [\n",
    "                    comment['snippet']['topLevelComment']['snippet']['textOriginal'] #to go bact to original comments use \"comment['snippet']['topLevelComment']['snippet']['textOriginal']\" without remove_emojis\n",
    "                    for comment in response['items']\n",
    "                ]\n",
    "\n",
    "                # Store each page of comments separately\n",
    "                all_comments.append({\n",
    "                    'video_id': video_id,\n",
    "                    'comments': comments_in_page\n",
    "                })\n",
    "\n",
    "                # Increment the page counter and update nextPageToken\n",
    "                page_count += 1\n",
    "                nextPageToken = response.get('nextPageToken')\n",
    "                if not nextPageToken:\n",
    "                    break\n",
    "\n",
    "        except HttpError as e:\n",
    "            print(f\"HttpError for video_id {video_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Combine all comments into a DataFrame\n",
    "    return pd.DataFrame(all_comments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to get the comments\n",
    "comments_df = get_comments(youtube, video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping comments by the video ID\n",
    "total_comments_df = comments_df.groupby('video_id')['comments'].agg(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming comments_df is your DataFrame with columns 'video_id' and 'comments'\n",
    "# Where 'comments' are lists of comments\n",
    "\n",
    "def concatenate_comment_lists(series):\n",
    "    # This function concatenates lists of comments into a single list\n",
    "    concatenated_list = []\n",
    "    for comment_list in series:\n",
    "        concatenated_list.append(comment_list)\n",
    "    return concatenated_list\n",
    "\n",
    "# Group by 'video_id' and concatenate the comments\n",
    "grouped_comments_df = comments_df.groupby('video_id')['comments'].apply(concatenate_comment_lists).reset_index()\n",
    "\n",
    "# Now, grouped_comments_df has each 'video_id' associated with a concatenated list of comments\n",
    "print(grouped_comments_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['viewCount', 'likeCount', 'favoriteCount', 'commentCount']\n",
    "video_details_df[numeric_cols] = video_details_df[numeric_cols].apply(pd.to_numeric, errors='coerce', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use code below to get publish day and time. Code above can be deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Published day of the Week\n",
    "\n",
    "video_details_df['publishedAt'] = video_details_df['publishedAt'].apply(lambda x: parse(x))\n",
    "video_details_df['publishDayName'] = video_details_df['publishedAt'].apply(lambda x: x.strftime('%A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is for seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isodate\n",
    "video_details_df['durationSeconds'] = video_details_df['duration'].apply(lambda x: isodate.parse_duration(x))\n",
    "video_details_df['durationSeconds'] = video_details_df['durationSeconds'].dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is for Minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df[['durationSeconds', 'duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting number of tags\n",
    "video_details_df['tagCount'] = video_details_df['tags'].apply(lambda x: len(x) if x is not None else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the video details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df.to_csv('video_details.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I'm just getting a quick visual representation a few stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highest Viewed Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'title', y = 'viewCount', data = video_details_df.sort_values('viewCount', ascending = False).head(9))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/100000) + 'K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(x = 'title', y = 'viewCount', data = video_details_df.sort_values('viewCount', ascending = True).head(9))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation = 90)\n",
    "ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/100000) + 'K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Violin Plot of Viewr Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=video_details_df['channelTitle'], y=video_details_df['viewCount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Views vs. Likes and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "sns.scatterplot(data = video_details_df, x = 'commentCount', y = 'viewCount', ax = ax[0])\n",
    "sns.scatterplot(data = video_details_df, x = 'likeCount', y = 'viewCount', ax = ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_counts = np.random.randint(100000, 10000000, 50)\n",
    "comment_counts = (view_counts / 100000) + np.random.normal(0, 10, 50)\n",
    "like_counts = (view_counts / 50) + np.random.normal(0, 1000, 50)\n",
    "\n",
    "# Create a DF from the data\n",
    "data = pd.DataFrame({\n",
    "    'viewCount': view_counts,\n",
    "    'commentCount': comment_counts,\n",
    "    'likeCount': like_counts\n",
    "})\n",
    "\n",
    "# Pearson correlation coefficients\n",
    "correlation_view_comment = data['viewCount'].corr(data['commentCount'])\n",
    "correlation_view_like = data['viewCount'].corr(data['likeCount'])\n",
    "\n",
    "correlation_view_comment, correlation_view_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average Video Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = video_details_df, x = 'durationSeconds', bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = STOPWORDS\n",
    "video_details_df['title_no_stopwords'] = video_details_df['title'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in video_details_df['title_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = ' '.join(all_words)\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\");\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 900, random_state=1, background_color='black', colormap='viridis', collocations=False).generate(all_words_str)\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = pd.DataFrame(video_details_df['publishDayName'].value_counts())\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_df = day_df.reindex(weekdays)\n",
    "ax = day_df.reset_index().plot.bar(x = 'publishDayName', y = 'count', rot = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI Saves to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_df.to_csv('openai.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('openai_comments.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurzgesagt Saves to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_details_df.to_csv('kurzgesagt.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments_df.to_csv('kurzgesagt_comments_original.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('kurzgesagt_comments.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
