{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "comments_df = pd.read_csv('kurzgesagt_comments_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we're tring to clean and tokenize the data then get our sentiment scores for each video**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting the tokenizer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of tokenizing a single comment\n",
    "inputs = tokenizer(\"I love this!\", return_tensors=\"pt\", truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "predictions = softmax(outputs.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the comments of puncuation and links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# The string to be removed\n",
    "remove_string = [\"offset carbon footprint wren \\u200b first 200 people sign kurzgesagt pay first month subscription video sponsored wren thanks lot support\",\n",
    "                \"'✨ WORLDWIDE SHIPPING AVAILABLE ✨ The 12,024 Human Era Calendar has landed! https://shop.kgs.link/12-024\\nJoin us on an exploration of how different cosmic conditions could shape unique worlds and civilizations.\\nStocks are highly limited, so don’t miss your chance to own a truly special piece of kurzgesagt.'\",\n",
    "                \"':sparkles: WORLDWIDE SHIPPING AVAILABLE :sparkles: The 12,024 Human Era Calendar has landed! Join us on an exploration of how different cosmic conditions could shape unique worlds and civilizations. Stocks are highly limited, so don’t miss your chance to own a truly special piece of kurzgesagt.'\",\n",
    "                \"Join us over on Discord to discuss and share your thoughts:\", \n",
    "                \"Go ‘beyond the nutshell’ at and dive deeper into these topics and more with a free 30-day trial! This video was sponsored by Brilliant. Thanks a lot for the support!\",\n",
    "                \"You want to learn more about science? Check out our sciency products on the kurzgesagt shop – all designed with love and produced with care. Getting something from the kurzgesagt shop is the best way to support us and to keep our videos free for everyone. ►► (Worldwide Shipping Available)\",\n",
    "                \"Head over to our shop to get exclusive kurzgesagt merch and sciency products designed with love. Getting something from the kurzgesagt shop is the best way to support us and to keep our videos free for everyone. ►► (Worldwide Shipping Available)\",]\n",
    "\n",
    "# Loop through all rows in the dataframe\n",
    "for index, row in comments_df.iterrows():\n",
    "    comment_str = row['comments']\n",
    "    try:\n",
    "        # Deserialize the comments for the current index\n",
    "        comment_list = ast.literal_eval(comment_str)\n",
    "\n",
    "        # Attempt to remove the unwanted string if it's in the list\n",
    "        for remove_string in comment_list:\n",
    "            comment_list.remove(remove_string)\n",
    "\n",
    "        # Serialize the list back and update the dataframe\n",
    "        comments_df.at[index, 'comments'] = str(comment_list)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean the text data while keeping emojis and URLs\n",
    "import re\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove newlines\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Keep emojis, lowercase the text, and remove unwanted punctuation\n",
    "    # No need to match emojis with a pattern, we are keeping them\n",
    "    text = text.lower()\n",
    "    # Remove punctuation (except for apostrophes and emojis)\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"\\,\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U00002702-\\U000027B0\\U000024C2-\\U0001F251]', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Clean the comments\n",
    "comments_df['cleaned_comments'] = comments_df['comments'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Show the cleaned comments\n",
    "comments_df[['comments', 'cleaned_comments']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get sentiment scores as a list\n",
    "def get_sentiment_score(comment):\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = softmax(outputs.logits, dim=-1)\n",
    "    return prediction.mean(dim=0).tolist()  # Averaging across all tokens\n",
    "\n",
    "# Clean the comments using the previously defined clean_text function\n",
    "\n",
    "# Get sentiment scores for each comment\n",
    "comments_df['sentiment_scores'] = comments_df['cleaned_comments'].apply(get_sentiment_score)\n",
    "\n",
    "# Now, split the sentiment_scores list into separate columns\n",
    "sentiment_columns = ['negative', 'neutral', 'positive']  # Adjust these names based on the model's sentiment labels\n",
    "for i, sentiment in enumerate(sentiment_columns):\n",
    "    comments_df[sentiment] = comments_df['sentiment_scores'].apply(lambda scores: scores[i])\n",
    "\n",
    "# Drop the 'sentiment_scores' column as it's no longer needed\n",
    "comments_df.drop('sentiment_scores', axis=1, inplace=True)\n",
    "\n",
    "# Group by video_id and calculate the mean sentiment scores\n",
    "video_sentiment_scores = comments_df.groupby('video_id')[sentiment_columns].mean()\n",
    "\n",
    "# Function to determine the label based on the highest mean score\n",
    "def get_video_sentiment_label(row):\n",
    "    max_score = max(row['negative'], row['neutral'], row['positive'])\n",
    "    if row['positive'] == max_score:\n",
    "        return 'Positive'\n",
    "    elif row['neutral'] == max_score:\n",
    "        return 'Neutral'\n",
    "    return 'Negative'  # If 'negative' has the highest score\n",
    "\n",
    "# Apply the function to determine the label for each video\n",
    "video_sentiment_scores['label'] = video_sentiment_scores.apply(get_video_sentiment_label, axis=1)\n",
    "\n",
    "# Convert the aggregated scores and labels to a DataFrame\n",
    "video_sentiment_df = video_sentiment_scores.reset_index()\n",
    "\n",
    "# Now you have a DataFrame with an overall sentiment and label for each video\n",
    "print(video_sentiment_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single Comment Test\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Example of tokenizing a single comment\n",
    "comment = \"'This theory was by far the most ideal and believable theory in the universe. I enjoyed this video very much. \\n\\nThank you for bringing extensive and complex theories to us in easier and understandable ways. You are one of the channels I adore.\\nKeep up with the fantastic videos, and I truly appreciate the commitment of your team. ❤'\"\n",
    "inputs = tokenizer(comment, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = softmax(outputs.logits, dim=1)\n",
    "\n",
    "# Get the highest probability index\n",
    "predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
    "\n",
    "# Convert index to label\n",
    "predicted_label = model.config.id2label[predicted_class_index]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Comment: '{comment}'\")\n",
    "print(f\"Predicted probabilities: {probabilities.tolist()[0]}\")\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming video_sentiment_df is your DataFrame and 'label' is the column with sentiment labels\n",
    "sentiment_counts = video_sentiment_df['label'].value_counts()\n",
    "\n",
    "# Create a pie chart\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Sentiment Distribution Across All Videos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the files for RoBerta model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sentiment_df.to_csv('kurzgesagt_video_sentiment_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('kurzgesagt_comments_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments_df = pd.read_csv('kurzgesagt_comments_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sentiment_df = pd.read_csv('kurzgesagt_video_sentiment_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiment = pd.merge(cleaned_comments_df, video_sentiment_df, on='video_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentiment = training_sentiment.drop(columns=['negative_y', 'neutral_y', 'positive_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of video IDs you want to focus on\n",
    "selected_video_ids = ['4_aOIA-vyBo', 'MUWUHf-rzks', 'NtQkz0aRDe8', 'EhAemz1v7dQ', 'PKMQzkIiB0Y', 'GDSf2h9_39I',\n",
    "'GoJsr4IwCm4', 'GqA42M4RtxE', 'Hug0rfFC_L8', 'IayvE_jFgrc', 'J0ldO87Pprc', 'JQVmkDUkZT4', 'Kr57ax0OWMk', 'LNv4y3wPQA0',\n",
    "'LBudghsdByQ', 'yiw6_JakZFc', 'ouAccsTzlGU', 'n3Xv_g3g-mA']\n",
    "\n",
    "# Filter the DataFrame to only include comments from those videos\n",
    "filtered_comments_df = training_sentiment[training_sentiment['video_id'].isin(selected_video_ids)]\n",
    "\n",
    "# Proceed to split this filtered DataFrame into training, validation, and test sets as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'filtered_comments_df' is your DataFrame\n",
    "# Ensure the 'label' column (or the appropriate column name for your labels) is converted to integers\n",
    "# Map string labels to integers\n",
    "filtered_comments_df.loc[:, 'label'] = filtered_comments_df['label'].replace(label_dict)\n",
    "\n",
    "\n",
    "\n",
    "# Now when you split your dataset\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    filtered_comments_df['cleaned_comments'], filtered_comments_df['label'], \n",
    "    test_size=0.3, random_state=42, stratify=filtered_comments_df['label']\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, \n",
    "#use stratify to make sure the distribution of labels is similar in all sets.\n",
    "    test_size=0.5, random_state=42, stratify=temp_labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_comments_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_comments_df['label'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class YouTubeCommentsDataset(Dataset):\n",
    "    def __init__(self, comments, labels, tokenizer, max_token_len=512):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_token_len = max_token_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "    \n",
    "    def __getitem__(self, item_index):\n",
    "        comment = self.comments[item_index]\n",
    "        label = self.labels[item_index]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            comment,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_token_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all labels are now integers\n",
    "assert filtered_comments_df['label'].apply(lambda x: isinstance(x, int)).all(), \"Not all labels are integers.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example mapping: adjust according to your data\n",
    "\n",
    "\n",
    "# Splitting the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = YouTubeCommentsDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
    "val_dataset = YouTubeCommentsDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n",
    "test_dataset = YouTubeCommentsDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have a GPU available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    num_labels=len(label_dict)  # Adjust based on your number of labels\n",
    ").to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "epochs = 10  # Define the number of epochs here\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "    model.train()\n",
    "    return accuracy_score(val_labels, val_preds)\n",
    "\n",
    "best_val_acc = 0\n",
    "early_stopping_counter = 0\n",
    "early_stopping_limit = 4  # Stop after 3 epochs of no improvement\n",
    "\n",
    "# Integrate training loop with validation and early stopping\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        model.zero_grad()  # Reset gradients to zero before starting backpropagation\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()  # Accumulate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate average loss over the epoch\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{epochs} | Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "    # Validation evaluation\n",
    "    val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}, Validation Accuracy: {val_acc}\")\n",
    "    model_save_path = './model_save'  # Define the directory to save the model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        early_stopping_counter = 0\n",
    "        # Save the model and its configuration\n",
    "        model.save_pretrained(model_save_path)\n",
    "        # Optionally, you can save the optimizer and scheduler state as well\n",
    "        torch.save({\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "        }, f'{model_save_path}/optimizer_scheduler_states.bin')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    if early_stopping_counter >= early_stopping_limit:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(true_labels))\n",
    "print(set(predictions))\n",
    "print(label_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_loader:\n",
    "    print(batch['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "        \n",
    "    logits = outputs.logits\n",
    "    predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "    true_labels.extend(batch['labels'].tolist())\n",
    "\n",
    "print(classification_report(true_labels, predictions, target_names=list(label_dict.keys())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Model and model embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_load_path = './model_save'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_load_path)\n",
    "\n",
    "optimizer_state = torch.load(f'{model_load_path}/optimizer_scheduler_states.bin')\n",
    "optimizer.load_state_dict(optimizer_state['optimizer_state_dict'])\n",
    "scheduler.load_state_dict(optimizer_state['scheduler_state_dict'])\n",
    "\n",
    "# Make sure to move the model to the correct device again\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "\n",
    "def predict(text, model, tokenizer):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return prediction  # Return the predicted class index\n",
    "\n",
    "# Example usage\n",
    "text = \"i'll check out those links though\"\n",
    "prediction = predict(text, model, tokenizer)\n",
    "print(f'Predicted class index: {prediction}')\n",
    "# You may want to map the predicted index to a label string based on your 'label_dict'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
