{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('PATH_TO_FILE.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\", disable = [\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the stop words from NLTK (you only need to do this once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get the stop words for English\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Now combine your custom stop words with NLTK's stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_stop_words = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = set(['your', 'the', 'of', 'a', 'and', 'get', 'would', 'im', 'dont', 'things', \"sure\", \"getting\", \"whats\", \"so\", \"theres\", \"go\", \"lets\", \"came\", \"thets\", \"btw\", \"something\", \"since\", \"said\", \"gotta\", \"shop\", \n",
    "                         \"see\", \"also\", \"cut\", \"could\", \"give\", \"even\", \"ago\", \"somehow\", \"aswell\", \"much\", \"it\", \"its\", \"it's\", \"isnt\", \"doesn\", \"ve\", \"ayo\", \"didnt\", \"really\", \"doesnt\", \"hi\", \"make\", \"stuff\", \"intervals\", \"look\", \"use\", \"purview\",\n",
    "                         \"next\", \"got\", \"asked\", \"bitches\", \"merch\", \"shipping\", \"product\", \"products\", \"buy\", \"ia\", \"lot\", \"first\", \"want\", \"though\", \"due\", \"whos\", \"daddy\", \"lifs\", \"month\", \"sorts\", \"other\", \"learnt\", \"gotten\", \"mrs\", \"around\", \"mincrafy\"\n",
    "                         \"minecraft\", \"always\", \"sus\", \"planck\", \"ever\", \"allow\", \"like\", \"hold\", \"else\", \"keep\", \"may\", \"irl\", \"lil\", \"says\", \"called\", \"usd\", \"thing\", \"hey\", \"wellyou\", \"ive\", \"mr\", \"firb\", \"try\", \"kinds\", \"makemake\",\n",
    "                         \"youre\", \"saw\", \"wish\", \"start\", \"display\", \"jobsthis\", \"never\", \"thats\", \"almost\", \"use\", \"don\", \"yet\", \"cok\", \"een\", \"heen\", \"st\", \"je\", \"naar\", \"anel\", \"co\", \"ik\", \"van\", \"en\", \"woord\", \"dit\", \"oh\", \"idea\", \"minimaal\",\n",
    "                         \"wouldn\", \"uitleggen\", \"gevoelens\", \"wel\", \"wouldnt\", \"wait\", \"pulling\", \"re\", \"come\", \"th\", \"is\", \"know\", \"my\", \"to\", \"can\", \"their\", \"but\", \"this\", \"theirs\", \"every\", \"in\", \"should\", \"show\", \"if\", \"that\", \"using\", \n",
    "                         \"notice\", \"been\", \"only\", \"made\", \"for\", \"are\", \"they\", \"by\", \"be\", \"as\", \"was\", \"not\", \"on\", \"do\", \"let\", \"still\", \"hard\", \"change\", \"well\", \"think\", \"point\", \"need\", \"might\", \"able\", \"last\", \"say\", \"back\", \"time\"\n",
    "                         \"actually\", \"one\", \"end\", \"rather\", \"reason\", \"thought\", \"say\", \"two\", \"become\", \"far\", \"gets\", \"basically\", \"away\", \"put\", \"took\", \"find\", \"past\", \"used\", \"matter\", \"instead\", \"must\", \"probably\", \"another\", \"makes\",\n",
    "                         \"tell\", \"enough\", \"many\", \"without\", \"going\", \"place\", \"etc\", \"maybe\", \"trying\", \"seen\", \"job\", \"espically\", \"absolutly\", \"started\", \"seeing\", \"everything\", \"already\", \"coming\", \"take\", \"thanks\", \"thank\", \"actually\",\n",
    "                         \"day\", \"done\", \"truly\", \"less\", \"stop\", \"understand\", \"small\", \"right\", \"went\", \"least\", \"months\", \"means\", \"amount\", \"goes\", \"found\", \"looking\", \"check\", \"light\", \"level\", \"anything\", \"absolutely\", \"free\", \"available\", \"video\",\n",
    "                         \"long\", \"possible\", \"book\", \"home\", \"part\", \"ones\", \"felt\", \"often\", \"times\", \"please\", \"support\", \"different\", \"entire\", \"real\", \"live\", \"later\", \"works\", \"bit\", \"exclusive\", \"chance\", \"form\", \"work\", \"watch\", \"feel\",\n",
    "                         \"guys\", \"human\", \"making\", \"feeling\", \"person\", \"money\", \"yes\", \"either\", \"full\", \"happen\", \"likely\", \"making\", \"easy\", \"head\", \"needs\", \"youtube\", \"today\", \"happened\", \"little\", \"concrete\", \"move\", \"finally\", \"detail\", \"nolan\",\n",
    "                         \"dev\", \"spook\", \"well\", \"nme\", \"notn\", \"bignnedit\", \"ihr\", \"nit\", \"daaa\", \"ways\", \"continue\", \"top\", \"knew\", \"coercion\", \"stands\", \"becoming\", \"stay\", \"force\", \"seems\", \"happens\", \"coe\", \"worth\", \"leaving\", \"stand\", \"quite\",\n",
    "                         \"among\", \"ferb\", \"fels\", \"yo\", \"look\", \"fard\", \"tells\", \"nnwell\", \"nnhumanity\", \"early\", \"tho\", \"wow\", \"glow\", \"rid\", \"glow\", \"nworker\", \"lactose\", \"decided\", \"nnperfectly\", \"max\", \"produces\", \"feeding\", \"gonna\", \"hands\",\n",
    "                         \"wanna\", \"whether\", \"asking\", \"wear\", \"gives\", \"shit\", \"loop\", \"ignore\", \"save\", \"nokay\", \"gave\", \"bullshit\", \"iii\", \"momey\", \"late\", \"alright\", \"sent\", \"lasted\", \"knowing\", \"pinky\", \"nbank\", \"turn\", \"lend\", \"dammit\", \"bro\",\n",
    "                         \"window\", \"stopped\", \"sake\", \"bell\", \"whatever\", \"thoroughly\", \"became\", \"moment\", \"select\", \"okay\", \"ityour\", \"range\", \"telling\", \"characters\", \"context\", \"giving\", \"satisfactionso\", \"height\", \"lag\", \"room\", \"reach\", \"bits\",\n",
    "                         \"source\", \"buggers\", \"kicked\", \"equivalent\", \"decipher\", \"mons\", \"input\", \"infrasound\", \"pile\", \"ringing\", \"looks\", \"ok\", \"assume\", \"taken\", \"wide\", \"plenty\", \"loved\", \"nnow\", \"pushing\", \"commonly\", \"suddenly\", \"input\", \"nnearth\",\n",
    "                         \"watched\", \"tntnndont\", \"puts\", \"couldnt\", \"oncennnn\", \"couldnt\", \"across\", \"fine\", \"puts\", \"core\", \"within\", \"round\", \"looked\", \"wrong\", \"literally\", \"running\", \"summary\", \"nothing\", \"tend\", \"ib\", \"built\", \"throwing\", \"multiple\",\n",
    "                         \"order\", \"slap\", \"talk\", \"browser\", \"kind\", \"somewhat\", \"exists\", \"resultsnnnthis\", \"picking\", \"peices\", \"preparing\", \"wie\", \"als\", \"zu\", \"wir\", \"das\", \"euch\", \"soo\", \"ich\", \"es\", \"ja\", \"seid\", \"aus\", \"cant\", \"cuz\", \"coz\", \"sit\", \"face\",\n",
    "                         \"recently\", \"constitutes\" \"fields\", \"helped\", \"told\", \"withnn\", \"proceed\", \"lives\", \"guess\", \"daily\", \"oh\", \"ah\", \"umm\", \"uh\", \"hmm\", \"hey\", \"okay\", \"ok\", \"class\", \"comes\", \"type\", \"putting\", \"starts\", \"pay\", \"non\", \"difference\",\n",
    "                         \"regular\", \"better\", \"sold\", \"access\", \"nnboy\", \"index\", \"prefer\", \"index\", \"dass\", \"gute\", \"finde\", \"noch\", \"ein\", \"theyll\", \"examples\", \"treat\", \"theyre\", \"sat\", \"takes\", \"examples\", \"showing\", \"bh\", \"seat\", \"alwaysnthis\", \"thisngreat\",\n",
    "                         \"wellnni\", \"itni\", \"holencould\", \"coverednwhat\", \"toonnwe\", \"throughfull\", \"individualnn\", \"nlot\", \"unerrated\", \"spit\", \"nbeauty\", \"ty\", \"nfrom\", \"nnso\", \"freehumanity\", \"lo\", \"tus\", \"de\", \"sheer\", \"eure\", \"kurz\", \"vid\", \"releasedi\",\n",
    "                         \"gboya\", \"set\", \"respekt\", \"beeindruckend\", \"cutnthe\", \"healthynthis\", \"foot\", \"auf\", \"auch\", \"besser\", \"pacing\", \"paddock\", \"placenn\", \"factorynno\", \"ftl\", \"thigs\", \"phil\", \"ideannit\", \"learnnnthanks\", \"sciencenwhile\",\n",
    "                         \"marvalhosos\", \"rigid\", \"trabalho\", \"nnfun\", \"nn\", \"ill\", \"aww\", \"fav\", \"wasnt\", \"feels\", \"setting\", \"concetto\", \"knowledgeall\", \"galaxynnthe\", \"grew\", \"saying\", \"schooler\", \"believe\", \"broken\", \"period\", \"hear\", \"affected\", \"inside\",\n",
    "                         \"attempt\", \"simple\", \"hats\", \"hearing\", \"points\", \"allows\", \"heard\", \"lol\", \"hours\", \"shout\", \"heren\", \"needed\", \"kinda\", \"stilltwitching\", \"pls\", \"id\", \"diameternnjust\", \"wont\", \"nnnthe\", \"arent\", \"xdd\", \"lean\", \"rock\", \"apparent\",\n",
    "                         \"mewn\", \"scrinn\", \"shipn\", \"meet\", \"given\", \"sooo\", \"seennand\", \"returnnoh\", \"gathering\", \"nnand\", \"uses\", \"sir\", \"tact\", \"step\", \"rolling\", \"rule\", \"salad\", \"pulls\", \"pull\", \"rlly\", \"dose\", \"nlooks\", \"yrs\", \"usable\", \"tshirtsni\",\n",
    "                         \"espically\", \"ur\", \"spot\", \"leads\", \"subscribersni\", \"leave\", \"chewing\", \"alwaysvery\", \"anymore\", \"knows\", \"damn\", \"formsnnthe\", \"npersonally\", \"awesomeyou\", \"favor\", \"realitiesif\", \"yk\", \"nnthey\", \"anyways\", \"moreso\", \"vu\", \"miss\", \"ti\",\n",
    "                         \"mean\", \"nnbut\", \"readerch\", \"drop\", \"thinking\", \"ink\", \"whatsoever\", \"pin\", \"objects\", \"vonnegut\", \"minute\", \"kurt\", \"kwumwesin\", \"alot\", \"thisll\", \"factsnawsome\", \"el\", \"xdnthe\", \"nthank\", \"sitting\", \"na\", \"la\", \"um\", \"boooom\",\n",
    "                         \"theyve\", \"fowleri\", \"usual\", \"birb\", \"keep\", \"keeps\", \"clicked\", \"warn\", \"improvednnalso\", \"cas\", \"tried\", \"netty\", \"sized\", \"fasciitis\", \"watern\", \"represent\", \"xd\", \"particular\", \"batches\", \"totally\", \"return\", \"bases\", \"nni\",\n",
    "                         \"obviously\", \"haha\", \"np\", \"bookthe\", \"ara\", \"nnni\", \"buti\", \"awhile\", \"xo\", \"willing\", \"prof\", \"lost\", \"nonnow\", \"ni\", \"fliesnoh\", \"kept\", \"shut\", \"badass\", \"basicscan\", \"nperson\", \"tnperson\", \"failsnin\", \"dmitriev\", \"fallneveryone\",\n",
    "                         \"mani\", \"worknthanks\", \"segway\", \"xdnalso\", \"tbh\", \"idk\", \"havent\", \"shall\", \"mentioned\", \"objeto\", \"rn\", \"trully\", \"haya\", \"hihi\", \"word\", \"toonrn\", \"stoooooop\", \"cutnmy\", \"yesnother\", \"agonnnormal\", \"nm\", \"mn\", \"lots\", \"met\",\n",
    "                         \"tn\", \"thisnnthank\", \"waynthis\", \"nalso\", \"historynmy\", \"nnfor\", \"worksnnif\", \"onenn\", \"nand\", \"hurry\", \"wayn\", \"nownnhow\", \"gone\", \"weve\", \"soon\", \"naught\", \"cantwont\", \"woooaah\", \"nanyway\", \"nwhat\", \"hmmm\", \"sont\", \"nday\",\n",
    "                         \"duh\", \"fuck\", \"usualy\", \"yesnnn\", \"backnni\", \"dosent\", \"mayb\", \"wayyyyyyyy\", \"backnni\", \"videosnnnnthe\", \"weaponsnnmost\", \"videonto\", \"universestuff\", \"rundown\", \"sooooo\", \"balltothewalls\", \"wants\", \"titlesnn\", \"anytime\", \"alonen\",\n",
    "                         \"nnnyep\", \"goodnnsuch\", \"nani\", \"livenni\", \"donapart\", \"til\", \"nncongress\", \"nnmilitary\", \"somebody\", \"worksnn\", \"yag\", \"powernnthat\", \"outputnn\", \"timesnn\"])\n",
    "\n",
    "all_stop_words = custom_stop_words.union(nltk_stop_words).union(spacy_stop_words)\n",
    "\n",
    "def preprocess(comment):\n",
    "    # This function processes a single comment.\n",
    "    tokens = gensim.utils.simple_preprocess(comment)\n",
    "    tokens = [token for token in tokens if token not in all_stop_words]\n",
    "    #doc = nlp(\" \".join(tokens))\n",
    "    #lammantized_tokens = [token.lemma_ for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV', 'PROPN']]\n",
    "    return tokens\n",
    "\n",
    "# Create a list to hold the data for the new DataFrame.\n",
    "processed_data = []\n",
    "\n",
    "# Iterate through each row of the existing comments_df DataFrame.\n",
    "for index, row in comments_df.iterrows():\n",
    "    video_id = row['video_id']\n",
    "    comments = row['cleaned_comments'].split(',')  # Split comments by comma\n",
    "    for comment in comments:\n",
    "        processed_comment = preprocess(comment)\n",
    "        processed_data.append({'video_id': video_id, 'processed_comment': processed_comment})\n",
    "\n",
    "# Create the new DataFrame from the list.\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Now processed_df contains a row for each comment, with the associated video_id.\n",
    "print(processed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating bigrams and trigrams\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = processed_df['processed_comment'].tolist()\n",
    "\n",
    "# Create bigrams and trigrams\n",
    "bigram = Phrases(tokenized_data, min_count=3, threshold=55)  # Higher threshold, fewer phrases.\n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "trigram = Phrases(bigram[tokenized_data], threshold=55)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# Now create bigrams and trigrams\n",
    "data_words_bigrams = make_bigrams(tokenized_data)\n",
    "data_words_trigrams = make_trigrams(tokenized_data)\n",
    "\n",
    "# If you want to store this data back in a DataFrame\n",
    "processed_df['bigrams'] = data_words_bigrams\n",
    "processed_df['trigrams'] = data_words_trigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processed_df['bigrams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional TF-IDF Removal\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Removal\n",
    "from gensim.models import TfidfModel\n",
    "id2word = gensim.corpora.Dictionary(processed_df['bigrams'])\n",
    "texts = processed_df['bigrams']\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "low_value = 0.03\n",
    "words = []\n",
    "words_missing_in_tfidf = []\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = []\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words + words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids]\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the comments by video_id\n",
    "grouped_comments = processed_df.groupby('video_id')['trigrams'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments.to_csv('grouped_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "grouped_comments_df = pd.read_csv('grouped_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test weighted words\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim\n",
    "from tqdm import tqdm  # Import tqdm\n",
    "\n",
    "video_topics = {}\n",
    "\n",
    "# Wrap the loop with tqdm for a progress bar\n",
    "for video_id, comments in tqdm(grouped_comments.items(), total=len(grouped_comments), desc=\"Processing videos\"):\n",
    "    # Create a dictionary and corpus for LDA\n",
    "    dictionary = corpora.Dictionary(comments)\n",
    "    corpus = [dictionary.doc2bow(comment) for comment in comments]\n",
    "\n",
    "    # Build the LDA model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=95, alpha=2.5, eta=0.068)\n",
    "\n",
    "    # Get the topics for this group of comments\n",
    "    topics = lda_model.print_topics(num_words=12)\n",
    "\n",
    "    # Store the topics in the video_topics dictionary\n",
    "    video_topics[video_id] = topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert video_topics to a DataFrame for easier viewing\n",
    "topics_df = pd.DataFrame(video_topics).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the topic model visualization\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary, mds = \"mmds\", R = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates wordcloud from the origional \"comments_df\" for comparision\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "stop_words = STOPWORDS\n",
    "comments_df['title_no_stopwords'] = comments_df['cleaned_comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in comments_df['title_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = ' '.join(all_words)\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\");\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 900, random_state=1, background_color='black', colormap='viridis', collocations=False).generate(all_words_str)\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordcloud for Processed_df, Not as important as the grouped_comments_df next\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = STOPWORDS\n",
    "processed_df['title_no_stopwords'] = processed_df['processed_comment'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])\n",
    "\n",
    "all_words = list([a for b in processed_df['title_no_stopwords'].tolist() for a in b])\n",
    "all_words_str = ' '.join(all_words)\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\");\n",
    "\n",
    "wordcloud = WordCloud(width = 1000, height = 900, random_state=1, background_color='black', colormap='viridis', collocations=False).generate(all_words_str)\n",
    "\n",
    "plot_cloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouped_comments_df that includess bigrmas and trigrams. This is the one to pay attention to.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Concatenate all trigrams into a single string\n",
    "all_trigrams_str = ' '.join(grouped_comments_df['trigrams'].tolist())\n",
    "\n",
    "# Function to plot the word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\")\n",
    "\n",
    "# Create a word cloud\n",
    "stop_words = STOPWORDS\n",
    "wordcloud = WordCloud(width=1000, height=900, random_state=1, background_color='black', colormap='viridis', collocations=False, stopwords=stop_words).generate(all_trigrams_str)\n",
    "\n",
    "# Plot the word cloud\n",
    "plot_cloud(wordcloud)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we're creating a wordcloud for each video individually.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_wordcloud(video_id):\n",
    "    # Filter for the specific video_id\n",
    "    specific_video_df = grouped_comments_df[grouped_comments_df['video_id'] == video_id]\n",
    "\n",
    "    # Concatenate all trigrams for this video into a single string\n",
    "    specific_video_trigrams_str = ' '.join(specific_video_df['trigrams'].tolist())\n",
    "\n",
    "    # Function to plot the word cloud\n",
    "    def plot_cloud(wordcloud):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    # Create a word cloud\n",
    "    stop_words = STOPWORDS\n",
    "    wordcloud = WordCloud(width=1000, height=900, random_state=1, background_color='black', colormap='viridis', collocations=False, stopwords=stop_words).generate(specific_video_trigrams_str)\n",
    "\n",
    "    # Plot the word cloud\n",
    "    plot_cloud(wordcloud)\n",
    "\n",
    "# Call the function with the desired video ID\n",
    "generate_wordcloud('RLykC1VN7NY')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
